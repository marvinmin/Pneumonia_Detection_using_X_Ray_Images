{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Detection using X-Ray Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to practice using CNN to process images and detect pneumonia based on the X-Ray images. Given the current COVID-19 pandemic, this project is both meaningful and interesting. Along with physical examination, imaging diagnosis plays a central role in the detection of pneumonia. In the chest X-Ray images, opacity areas are often correlated to pneumonia affected regions. However, the identification of opacity areas in chest X-Ray images is sometimes challenging. Machine learning and artificial intelligence can be used to detect pneumonia based on chest X-Ray images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this project is an adapted version of dataset submitted by Paul Mooney at [Kaggle](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia), which contains a more balanced train-validation-test split of the data. In total, there are 5856 observations in the dataset that is split into 4192 training examples (1082 normal and 3110 opacity), 1040 validation examples (267 normal and 773 opacity), and 624 testing examples (234 normal and 390 opacity).\n",
    "\n",
    "All the chest X-ray images were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients routine clinical care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with traditional machine learning libraries, Keras image preprocessing ([ImageDataGenerator](https://keras.io/api/preprocessing/image/#imagedatagenerator-class)) and deep learning objects (Sequential, Conv2D, MaxPooling2D, Flatten, Dense) are used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Some hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'hyper_dimension': target image width and length in pixels considered when original images need to be rescaled for processing;\n",
    "- 'hyper_epochs': number of epochs (leaning iterations through which the whole dataset is exposed to the machine for weight updates);\n",
    "- 'hyper_batch_size': size of image batches;\n",
    "- 'hyper_feature_maps': reference number of feature maps generated by convolutional layers;\n",
    "- 'hyper_channels' and 'hyper_mode': number of channels utilized in the learning process. For colored RGB images, hyper_channels = 3 and hyper_mode = 'rgb', yet for grayscale images hyper_channels = 1 and hyper_mode = 'grayscale'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dimension = 500\n",
    "hyper_epochs = 100\n",
    "hyper_batch_size = 16\n",
    "hyper_feature_maps = 32\n",
    "hyper_channels = 1\n",
    "hyper_mode = 'grayscale'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep learning- Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create and compile the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\minch\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\minch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\minch\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Convolution & pooling - First convolution layer\n",
    "classifier.add(Conv2D(hyper_feature_maps, (3, 3),\n",
    "                      input_shape = (hyper_dimension,\n",
    "                                     hyper_dimension,\n",
    "                                     hyper_channels),\n",
    "                      activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Convolution & pooling - Second convolution layer (same as first layer)\n",
    "classifier.add(Conv2D(hyper_feature_maps, (3, 3),\n",
    "                      input_shape = (hyper_dimension,\n",
    "                                     hyper_dimension,\n",
    "                                     hyper_channels),\n",
    "                      activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Convolution & pooling - Third convolution layer\n",
    "classifier.add(Conv2D(hyper_feature_maps * 2, (3, 3),\n",
    "                      input_shape = (hyper_dimension,\n",
    "                                     hyper_dimension,\n",
    "                                     hyper_channels),\n",
    "                      activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Full connection\n",
    "classifier.add(Dense(units = hyper_feature_maps * 2, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
